---
title: Processing Pipeline
---

# Processing Pipeline

The `.readDoc()` API, when used with the default instance of `winkNLP`, splits the text into tokens, entities, and sentences. It also determines a range of their properties. These are accessible via the `.out()` API on the basis of the input parameter — <code>its.<strong>property</strong></code>. Some examples of properties are `value`, `stopWordFlag`, `pos`, and `lemma`:


<pre class="prettyprint source lang-javascript"><code>const text = 'cats are cool';
const doc = nlp.readDoc( text );
console.log( doc.tokens().out( its.value) );
// -> ["cats", "are", "cool"]
console.log( doc.tokens().out( its.stopWordFlag ) );
// -> [false, true, false]
console.log( doc.tokens().out( its.pos ) );
// -> ["NOUN", "AUX", "VERB"]
console.log( doc.tokens().out( its.lemma ) );
// -> ["cat", "be", "cool"]</code></pre>


The `.readDoc()` API processes the input text in many stages. All the stages together form a processing pipeline or a pipe. The first stage is __tokenization__, which is mandatory. The later stages such as sentence boundary detection or pos tagging are optional. The optional stages are user configurable. The following figure & table illustrates the pipe:


Common its.__properties__ that become available at each stage are also highlighted. For more details please refer to sections on [item and its properties](https://winkjs.org/wink-nlp/item.html) and [its & as helpers](https://winkjs.org/wink-nlp/its-as-helper.html).

<table>
  <tr>
    <th>Stage</th>
    <th>Description</th>
  </tr>

  <tr>
    <td><code>tokenization</code></td>
    <td>Splits text into tokens.</td>
  </tr>

  <tr>
    <td><code>sbd</code></td>
    <td>Sentence boundary detection — determines **span** of each sentence in terms of start & end token indexes.</td>
  </tr>

  <tr>
    <td><code>negation</code></td>
    <td>Negation handling — sets the <code>negationFlag</code> for every token whose meaning is negated due a "not" word.</td>
  </tr>

  <tr>
    <td><code>sentiment</code></td>
    <td>Computes sentiment score of each sentence and the entire document.</td>
  </tr>

  <tr>
    <td><code>ner</code></td>
    <td>Named entity recognition — detects all named entities and also determines their type & span.</td>
  </tr>

  <tr>
    <td><code>pos</code></td>
    <td>Performs part-of-speech (pos) tagging.</td>
  </tr>

  <tr>
    <td><code>cer</code></td>
    <td>Custom entity recognition — detects all custom entities and their type & span. The detection is carried out on the basis of training carried out using <a href="custom-entities.html"><code>learnCustomEntities()</code></a> method.</td>
  </tr>
</table>

The default instance of `winkNLP` is created using only the `model` as input parameter:

<pre class="prettyprint source lang-javascript"><code>// Load wink-nlp package.
const winkNLP = require( 'wink-nlp' );
// Load english language model — light version.
const model = require( 'wink-eng-lite-model' );
// Instantiate winkNLP — default — will run all the above mentioned
// stages.
const nlp = winkNLP( model );</code></pre>

It also accepts an additional parameter — `pipe` that controls the processing pipeline. This parameter is an array that contains the names of the stages that you wish to run. For example, the following will only run **sentence boundary detection** and __pos tagging__ after tokenization:

<pre class="prettyprint source lang-javascript"><code>const nlp = winkNLP( model, [ 'sbd', 'pos' ] );</code></pre>

The interplay between stages and properties is outlined below:
1. While the sequence of stages in a pipe is not important as `winkNLP` handles it automatically, it is recommended to always provide names in the correct logical sequence.
1. Without `sbd`, the entire text is treated as a single sentence.
1. `sentiment` is dependent on `negation`; without negation, the accuracy of sentiment score may drop.
1. Without `pos`, `its.lemma` accuracy drops drastically.
1. Without `ner`, the count of named entities will always be zero i.e. `doc.entities().length()` will return a zero.
1. Without `cer`, the count of custom entities will always be zero i.e. `doc.customEntities().length()` will return a zero.


<div class="docs-tip docs-tip--yellow">
  Running only required stages can give a performance advantage. For example only "tokenization" runs at the speed of 2.1 million tokens per second. And "tokenization+sbd" delivers speed of about 1.5 million tokens per second. Contrast it with the speed of 0.5 million tokens per second, when all the stages are active.
</div>


<div class="l-horizontal">
  <a href="getting-started.html" class="bottom-nav-link">
    <span class="bottom-nav-link__title">Previous</span>
    Getting started
  </a>

  <a href="document.html" class="bottom-nav-link next-nav-link">
    <span class="bottom-nav-link__title">Next</span>
    Document
  </a>
</div>
